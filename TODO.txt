
Import history tables recreate contracts table in snowflake measure consumption and duration



Finish CLI
    - Don't get to hung up on best way to implement just do a fucking if else chain of death

Abstract away views from source db to management db
    - specify server (specifies where all jobs will be pulling from) (management db will always be same name)
        - server can live in the Job class and propogate to the Source class
    - specify databases/tables/batches to scrape (do same join except cross db)

For incrementals we have some decisions:
    - Do we assume all incrementals will be < 3M rows? -- I'm thinking this is best????????????
    - Or do we do same hashing and just add in an additional filter?
    - Abstract the table meta class to get incremental row counts? Table scans are expensive


Optimize the filter to reduce db compute, implement incremental filter



